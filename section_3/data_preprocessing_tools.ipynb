{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 nan]\n",
      " [35.0 58000.0]\n",
      " [nan 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `SimpleImputer`: class provided by scikit-learn's sklearn.impute module. It is used for imputing missing values in datasets. Imputation is the process of replacing missing data with substituted values. For the case above:\n",
    "\n",
    "    * `missing_values`: allows you to specify the value or values that represent missing data in your dataset. When the SimpleImputer encounters these values in the dataset, it will consider them as missing and perform imputation based on the specified strategy.\n",
    "\n",
    "    * `strategy`: specifies the imputation strategy to be used for filling missing values in the dataset. It determines how the missing values are replaced or filled. For this case, `mean` replaces missing values with the mean of the non-missing values for each feature (column).\n",
    "\n",
    "2. The `fit()` method of `SimpleImputer()` calculates the necessary statistics from the data needed for imputation based on the specified strategy. For example, if you use the 'mean' strategy, it calculates the mean of each feature (column) from the data. The method takes the dataset (or a subset of it) as input.\n",
    "\n",
    "3. The `transform()` method of `SimpleImputer()` is a method used to transform the dataset by imputing missing values based on the fitted imputer object. After fitting the `SimpleImputer` object to the data using the `fit()` method, you can use the `transform()` method to replace missing values in the dataset with the imputed values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `ColumnTransformer` is a class provided by scikit-learn that allows you to apply different transformers to different columns of your dataset. It's particularly useful when you have a dataset with mixed data types (e.g., numerical and categorical features) and you want to apply different preprocessing steps to each type of feature.\n",
    "\n",
    "    * `transformers`: This parameter specifies the list of transformers to be applied to the columns of the dataset. It should be a list of tuples, where each tuple contains:\n",
    "\n",
    "        a. A name for the transformer (optional). For this case, `encoder`\n",
    "\n",
    "        b. The transformer object itself. For this case, `OneHotEncoder()`\n",
    "        \n",
    "        c. The column indices (or column names) to which the transformer should be applied. For this case, it will be the first column of the dataset (`[0]`)\n",
    "\n",
    "    * `remainder`: This parameter specifies what to do with the columns that are not specified in the transformers list. It can take one of the following values:\n",
    "\n",
    "        a. `drop`: Drop the remaining columns that are not transformed.\n",
    "\n",
    "        b. `passthrough`: Leave the remaining columns unchanged and include them in the output.\n",
    "\n",
    "        c. A transformer object: Apply the specified transformer to the remaining columns.\n",
    "\n",
    "2. `fit_transform(X)`: This part of the code fits the `ColumnTransformer` object `ct` to the dataset `X` and then transforms `X` according to the specified transformations. The `fit_transform()` method combines the fitting (learning the parameters) and transformation steps. It applies the transformations defined in the ColumnTransformer to the columns of X based on the rules specified in the transformers parameter. After transformation, it returns the transformed dataset.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 nan]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 nan 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***One-hot Encoding:*** s a technique used in machine learning and data preprocessing to represent categorical variables as binary vectors, i.e., [0,0,1,0] or [1,0,0,0], etc. In one-hot encoding, each categorical variable is represented as a binary vector with a length equal to the number of unique categories in the variable. Each category is then represented by a binary value: 1 if the sample belongs to that category, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lc = LabelEncoder()\n",
    "y = lc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `LabelEncoder` is a utility class in scikit-learn used for encoding categorical labels into numerical labels. It assigns a unique integer to each unique category in the input data, thereby converting categorical labels into numerical representations.\n",
    "\n",
    "2. The `fit_transform()` method of LabelEncoder is called with the target variable y as input. This method fits the LabelEncoder to the unique categories present in y and transforms the categories into their corresponding numerical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Difference between `LabelEncoder()` and `ColumnTransformer()`:*** \n",
    "\n",
    "Both components of scikit-learn used for data preprocessing, but they serve different purposes and operate on different types of data.\n",
    "\n",
    "1. `LabelEncoder()`:\n",
    "    * Used to encode categorical labels into numerical labels.\n",
    "    * It is typically applied to a single feature (column) containing categorical labels.\n",
    "    * Assigns a unique integer to each unique category in the input data, thereby converting categorical labels into numerical representations.\n",
    "    * It is commonly used for encoding target variables in classification tasks or transforming categorical labels for certain preprocessing tasks.\n",
    "\n",
    "2. `ColumnTransformer()`:\n",
    "    * Used for applying different transformations to different columns (features) of a dataset.\n",
    "    * It allows you to specify different preprocessing steps for different columns of the dataset, including transformations like scaling, encoding, imputation, and more.\n",
    "    * ColumnTransformer() is more versatile and can handle various preprocessing tasks, including encoding categorical variables using techniques like `OneHotEncoder()` or `OrdinalEncoder()`.\n",
    "    * It is often used as part of a preprocessing pipeline to apply different transformations to different subsets of the dataset.\n",
    "\n",
    "In summary, the main difference between `LabelEncoder()` and `ColumnTransformer()` is their scope and purpose: LabelEncoder() is specifically designed for encoding categorical labels, while ColumnTransformer() is a more general-purpose tool for applying transformations to multiple columns of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the dataset into the Trainning set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `train_test_split()` is a function from the sklearn.model_selection module in scikit-learn. It is used for splitting a dataset into two or more subsets for training and testing machine learning models.\n",
    "\n",
    "    * The first parameter are the arrays from the dataset you want to split. In this case, `X` and `y`\n",
    "    * `test_size`: This parameter specifies the proportion of the dataset that should be included in the test split. It can be either a float (representing the proportion) or an integer (representing the absolute number of samples).\n",
    "    * `random_state`: This parameter controls the random seed used for shuffling the dataset before splitting. Setting a specific random state ensures reproducibility of the split.\n",
    "    * The function returns a tuple containing the split datasets. By default, it returns the training data, testing data, training labels, and testing labels (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_udemy_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
